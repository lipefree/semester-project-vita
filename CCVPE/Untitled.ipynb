{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1f762526-23a7-4a82-8f87-407201f4e205",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/qngo/venvs/orienternet/lib/python3.9/site-packages/mmengine/optim/optimizer/zero_optimizer.py:11: DeprecationWarning: `TorchScript` support for functional optimizers is deprecated and will be removed in a future PyTorch release. Consider using the `torch.compile` optimizer instead.\n",
      "  from torch.distributed.optim import \\\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from mmcv.ops import MultiScaleDeformableAttention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "28dc7fa6-9ff2-4d31-b730-e58d46434915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: torch.Size([3, 262144, 256])\n"
     ]
    }
   ],
   "source": [
    "# Define the module\n",
    "embed_dims = 256  # Embedding dimension\n",
    "num_heads = 8\n",
    "num_levels = 2  # Two levels for multi-scale attention\n",
    "num_points = 4\n",
    "\n",
    "attention = MultiScaleDeformableAttention(\n",
    "    embed_dims=embed_dims,\n",
    "    num_heads=num_heads,\n",
    "    num_levels=num_levels,\n",
    "    num_points=num_points,\n",
    "    batch_first=True  # Assuming batch dimension comes first\n",
    ")\n",
    "\n",
    "# Input tensors\n",
    "bs = 3  # Batch size\n",
    "h, w = 512, 512  # Spatial dimensions of the original feature map\n",
    "\n",
    "# Query and key-value tensors\n",
    "query = torch.rand(bs, h * w, embed_dims)  # Query reshaped to (batch, num_query, embed_dims)\n",
    "key_value_layer1 = torch.rand(bs, (h // 2) * (w // 2), embed_dims)  # First layer: Half resolution\n",
    "key_value_layer2 = torch.rand(bs, h * w, embed_dims)  # Second layer: Original resolution\n",
    "\n",
    "# Combine key and value tensors\n",
    "key_value = torch.cat([key_value_layer1, key_value_layer2], dim=1)  # Concatenate along the query dimension\n",
    "\n",
    "# Spatial shapes for two layers\n",
    "spatial_shapes = torch.tensor([\n",
    "    [h // 2, w // 2],  # First layer (reduced resolution)\n",
    "    [h, w]             # Second layer (original resolution)\n",
    "], dtype=torch.int32)  # Shape: (num_levels, 2)\n",
    "\n",
    "# Reference points (normalized) for two levels\n",
    "reference_points = torch.rand(bs, query.size(1), num_levels, 2)  # Random points for 2 levels\n",
    "\n",
    "# Level start indices (start positions for each level in the concatenated key_value tensor)\n",
    "level_start_index = torch.tensor([\n",
    "    0,                              # Start of level 0 (first layer)\n",
    "    (h // 2) * (w // 2)             # Start of level 1 (second layer)\n",
    "], dtype=torch.int32)\n",
    "\n",
    "# Forward pass through the attention module\n",
    "output = attention.forward(\n",
    "    query=query,\n",
    "    key=key_value,\n",
    "    value=key_value,\n",
    "    reference_points=reference_points,\n",
    "    spatial_shapes=spatial_shapes,\n",
    "    level_start_index=level_start_index\n",
    ")\n",
    "\n",
    "# Output\n",
    "print(\"Output shape:\", output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcca628d-76b7-4b42-9b61-1ec6efd3cad9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 327680, 256])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6fd98a06-233d-4ebd-9be1-60e49def78ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 65536, 256])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key_value_layer1.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc2d95c-31b3-495c-8fea-f9ce3b477651",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "orienternet",
   "language": "python",
   "name": "orienternet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
